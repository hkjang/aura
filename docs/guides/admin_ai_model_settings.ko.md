# 관리자 AI 모델 설정 상세 가이드

이 문서는 **관리자(ADMIN)** 권한을 가진 사용자가 Aura 포털에서 다양한 AI 모델(OpenAI, Ollama, vLLM)을 연결하고 관리하는 방법을 상세하게 설명합니다.

**메뉴 위치:** `시스템 > 설정`

---

## 1. 개요

Aura 포털은 멀티 모델 아키텍처를 지원합니다. 관리자는 필요에 따라 여러 모델을 등록하고 각 모델의 연결 정보를 개별적으로 관리할 수 있습니다. 등록된 모델은 일반 사용자가 채팅 메뉴에서 선택하여 사용할 수 있습니다.

**주요 설정 항목:**

- **표시 이름 (Display Name)**: 사용자에게 보여질 모델의 이름 (예: "빠른 챗봇", "코드 전문가").
- **제공자 (Provider)**: 모델 서비스 공급자 (`openai`, `ollama`, `vllm`).
- **모델 ID (Model ID)**: API 호출 시 사용되는 실제 모델 식별자.
- **Base URL**: 자체 호스팅 서버 등을 사용할 때 API 엔드포인트 주소.
- **API Key**: 서비스 인증을 위한 보안 키.

---

## 2. 모델 추가하기

설정 화면 하단의 **"새 모델 추가"** 폼을 사용합니다.

### A. OpenAI 모델 추가 (ChatGPT)

클라우드 기반의 최신 GPT 모델을 연결할 때 사용합니다.

1. **표시 이름**: `GPT-4 Turbo` (원하는 이름 입력)
2. **제공자**: `OpenAI` 선택
3. **모델 ID**: `gpt-4-turbo-preview` 또는 `gpt-3.5-turbo`
   - _참고: OpenAI 공식 문서에서 최신 모델 ID를 확인하세요._
4. **Base URL**: (비워둠)
   - _참고: Azure OpenAI를 사용하는 경우 Azure 엔드포인트를 입력해야 할 수 있습니다._
5. **API 키**: OpenAI 콘솔에서 발급받은 `sk-...` 로 시작하는 키 입력.
6. **[모델 추가]** 버튼 클릭.

### B. Ollama 모델 추가 (로컬/오프라인)

사내 서버나 로컬 PC에 Ollama를 설치하여 운영하는 경우입니다. 보안이 중요한 폐쇄망 환경에서 권장됩니다.

1. **사전 준비**: Ollama 서버가 실행 중이어야 하며, Aura 서버에서 해당 주소로 접근 가능해야 합니다.
   - 기본 포트: `11434`
   - 환경 변수 `OLLAMA_ORIGINS="*"` 설정 권장 (CORS 문제 방지).
2. **표시 이름**: `Llama 3 (로컬)`
3. **제공자**: `Ollama` 선택
4. **모델 ID**: `llama3`, `mistral`, `gemma` 등 Ollama 라이브러리의 모델명.
5. **Base URL**: `http://localhost:11434/v1` 또는 `http://<서버IP>:11434/v1`
   - **주의**: 반드시 끝에 `/v1`을 붙여야 OpenAI 호환 모드로 동작합니다.
6. **API 키**: (비워둠)
   - _참고: Ollama는 기본적으로 인증이 없으나, 별도 프록시를 쓴다면 입력._
7. **[모델 추가]** 버튼 클릭.

### C. vLLM 모델 추가 (고성능 추론 서버)

자체 GPU 서버에서 vLLM을 사용하여 고성능으로 모델을 서빙하는 경우입니다.

1. **표시 이름**: `Internal Code LLM`
2. **제공자**: `vLLM` 선택
3. **모델 ID**: vLLM 서버 실행 시 `--model` 옵션으로 지정한 모델 경로 또는 이름.
   - 예: `meta-llama/Meta-Llama-3-70B-Instruct`
4. **Base URL**: `http://<GPU서버IP>:8000/v1`
   - vLLM 기본 포트는 8000입니다. 주소 끝에 `/v1`을 포함하세요.
5. **API 키**: vLLM 실행 시 `--api-key`를 설정했다면 입력, 아니면 비워둡니다.
6. **[모델 추가]** 버튼 클릭.

---

## 3. 모델 관리

### 모델 수정

1. **활성 모델** 목록에서 수정하려는 모델의 우측 **설정(톱니바퀴) 아이콘**을 클릭합니다.
2. 하단 폼에 해당 모델의 정보가 로드됩니다.
3. 필요한 정보를 수정합니다.
   - **주의**: 보안을 위해 기존 API 키는 표시되지 않습니다. API 키를 변경하지 않으려면 비워두고, 변경하려면 새 키를 입력하세요.
4. **[수정 사항 저장]** 버튼을 클릭합니다.

### 모델 삭제

1. **활성 모델** 목록에서 삭제하려는 모델의 우측 **삭제(휴지통) 아이콘**을 클릭합니다.
2. 확인 팝업에서 "확인"을 선택하면 즉시 삭제됩니다.
   - _참고: 삭제된 모델의 과거 대화 이력은 유지되지만, 더 이상 새 대화 생성 시 선택할 수 없습니다._

---

## 4. 문제 해결 (Troubleshooting)

### Q: "연결 실패" 또는 "오류 발생" 메시지가 뜹니다.

- **Base URL 확인**: 주소 끝에 `/v1`이 포함되어 있는지, `http://` 또는 `https://` 프로토콜이 정확한지 확인하세요.
- **네트워크 확인**: Aura 서버에서 모델 서버(Ollama/vLLM) IP로 `ping`이나 `curl` 요청이 가능한지 네트워크 담당자에게 문의하세요.
- **Docker 사용 시**: Aura가 도커 컨테이너 내부에서 실행 중이고 Ollama가 호스트에서 실행 중이라면, `localhost` 대신 `host.docker.internal`을 사용해야 할 수 있습니다.

### Q: 모델이 답변을 멈추거나 타임아웃이 발생합니다.

- 모델 서버의 GPU 메모리가 부족하거나 과부하 상태인지 확인하세요.
- 프롬프트가 너무 길어 모델의 컨텍스트 윈도우(Context Window)를 초과했는지 확인하세요.

### Q: 사용자가 모델 목록에서 내가 추가한 모델을 볼 수 없습니다.

- 모델 추가 후 페이지를 새로고침 해보세요.
- 데이터베이스에 정상적으로 저장되었는지 서버 로그를 확인하세요.
