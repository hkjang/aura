# 모델 비교 (Model Comparison) 가이드

이 문서는 Aura 포털의 **모델 비교** 기능을 사용하여 작업에 최적화된 AI 모델을 선택하는 방법을 설명합니다.

**메뉴 위치:** `AI 사용 > 모델 비교`

---

## 1. 개요

Aura는 OpenAI, Anthropic, Google, 그리고 사내 로컬 모델(Llama, Mistral) 등 다양한 모델을 지원합니다. 동일한 프롬프트(질문)를 여러 모델에 동시에 전송하여 응답 속도, 품질, 비용 등을 정량적으로 비교할 수 있습니다.

---

## 2. 사용 방법

### 2.1 아레나(Arena) 설정

좌측 패널에서 비교할 선수(모델)들을 입장시킵니다.

- **상용 모델**: GPT-4, Claude 3.5 Sonnet (높은 지능, 비용 발생)
- **로컬 모델**: Llama 3 70B, Mixtral 8x7B (보안 우수, 무료/저비용)

### 2.2 테스트 파라미터 통일

공정한 비교를 위해 변인을 통제합니다.

- **Temperature**: 창의성 조절 (0.0 ~ 1.0). 낮을수록 사실적, 높을수록 창의적.
- **비교 모드**: '단판 승부' 또는 '멀티턴 대화'를 선택합니다.

---

## 3. 결과 분석 지표

결과 카드 하단의 수치를 통해 모델의 효율성을 판단하세요.

### 성능 지표

- **TTFT (Time To First Token)**: 질문 후 첫 글자가 나올 때까지 걸린 시간. 사용자 체감 속도에 가장 중요합니다. (0.5초 미만 권장)
- **TPS (Tokens Per Second)**: 초당 생성 속도. 높을수록 긴 글을 빨리 써냅니다.
- **Total Latency**: 답변이 완전히 끝날 때까지 걸린 총 시간.

### 품질 지표

- **정확성**: 팩트 체크가 필요한 질문에서 환각(Hallucination)이 없는지 확인.
- **지시 이행력 (Instruction Following)**: "세 문장으로 요약해", "JSON 포맷으로 줘" 같은 제약 조건을 잘 지키는지 확인.

---

## 4. 비용 분석 (Cost Benefit)

| 모델 등급                 | 평균 비용 ($/1M 토큰) | 추천 용도                               |
| :------------------------ | :-------------------- | :-------------------------------------- |
| **High-End** (GPT-4급)    | $10 ~ $30             | 복잡한 코딩, 계약서 검토, 전략 기획     |
| **Mid-Range** (GPT-3.5급) | $0.5 ~ $3             | 단순 요약, 챗봇, 번역, 이메일 작성      |
| **Local / Open**          | $0 (전기세 제외)      | 기밀 데이터 처리, 대량의 단순 반복 작업 |

---

## 5. 실전 팁

- **블라인드 테스트**: 선입견을 없애기 위해 '모델명 숨기기' 옵션을 켜고 답변 품질만으로 평가해보세요.
- **엣지 케이스 테스트**: 모델이 헷갈릴 만한 논리 퀴즈나 모호한 질문을 던져서 위기 대처 능력을 봅니다.
